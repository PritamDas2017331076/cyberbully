{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "bbg_R6QGtu81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "1qQvaeq_woO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "6agcr4rlYTyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLbCI2hLKd7T"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import gensim\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Sequential,Model,load_model\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Conv2D, concatenate, MaxPooling2D, Input, Flatten, Reshape\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WvOv9BHgvroM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "4viiNN9CwmCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "SbW4Sof9zyCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bully_df = pd.read_csv('./drive/MyDrive/datasets/cyberbullying_tweets.csv')"
      ],
      "metadata": {
        "id": "xAtsEEfmKqpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleantxt(txt):\n",
        "    stpw = stopwords.words('english')\n",
        "\n",
        "    stpw.extend(['www','http','utc'])\n",
        "    stpw = set(stpw)\n",
        "    txt = re.sub(r\"\\n\", \" \", txt)\n",
        "    txt = re.sub(\"[\\<\\[].*?[\\>\\]]\", \" \", txt)\n",
        "    txt = txt.lower()\n",
        "    txt = re.sub(r\"[^a-zA-Z ]\", \" \", txt)\n",
        "    txt = re.sub(r\"\\b\\w{1,3}\\b\", \" \",txt)\n",
        "    txt = \" \".join([x for x in txt.split() if x not in stpw])\n",
        "    return txt"
      ],
      "metadata": {
        "id": "L3Adpqa2Kbk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bully_df.tail()"
      ],
      "metadata": {
        "id": "qv__oDjwLrpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bully_df = bully_df.rename(columns={'tweet_text':'tweet','cyberbullying_type':'type'})"
      ],
      "metadata": {
        "id": "rK7eGalzMyI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bully_df.head()"
      ],
      "metadata": {
        "id": "IgRWiOquNWe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bully_df['tweet'] = bully_df['tweet'].apply(cleantxt)"
      ],
      "metadata": {
        "id": "78N_Ju5OmvEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bully_df.head()"
      ],
      "metadata": {
        "id": "2i8p2hO4nYqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bully_df.tail(20)"
      ],
      "metadata": {
        "id": "qMCU1zSdKfoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_sentence(sentence):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in tokens]\n",
        "    return ' '.join(lemmatized_words)"
      ],
      "metadata": {
        "id": "X-PI3cyswfeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bully_df['tweet'] = bully_df['tweet'].apply(lemmatize_sentence)"
      ],
      "metadata": {
        "id": "KW50QMqYxbft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bully_df.tail(10)"
      ],
      "metadata": {
        "id": "xmEJCFYTyveB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "negative_words = ' '.join([text for text in bully_df['tweet'][bully_df['type'] != 'not_cyberbullying']])\n",
        "wordcloud = WordCloud(width=800, height=500,\n",
        "random_state=21, max_font_size=110).generate(negative_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dizrm6CF2kC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad_word = ['fuck','dumb','black','nigger','terrorist','bully','stupid','rape','joke','racism','islam','idiot','isis','bitch','hate','racist','kill','sexist','shit','whore','terrorism']"
      ],
      "metadata": {
        "id": "AAxe1rddDdaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bully_df['tweetsplit'] = bully_df['tweet'].apply(lambda x: x.split())\n",
        "# bully_df['tweet'] = bully_df['tweet'].apply(lambda x: ([w for w in x if len(w)>=3]))\n",
        "# bully_df['tweetcnn'] = bully_df['tweet'].apply(lambda x: ' '.)"
      ],
      "metadata": {
        "id": "QYcyGTAn5RPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bully_df.tail(20)"
      ],
      "metadata": {
        "id": "sW5JdP8n5ang"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gender religion age ethnicity other_cyberbullying\n",
        "not_bully_df = bully_df[bully_df['type'] == 'not_cyberbullying']\n",
        "gender_bully_df = bully_df[bully_df['type'] == 'gender']\n",
        "religion_bully_df = bully_df[bully_df['type'] == 'religion']\n",
        "age_bully_df = bully_df[bully_df['type'] == 'age']\n",
        "ethnicity_bully_df = bully_df[bully_df['type'] == 'ethnicity']\n",
        "other_bully_df = bully_df[bully_df['type'] == 'other_cyberbullying']\n",
        "all_bully_df = bully_df[bully_df['type'] != 'not_cyberbullying']\n",
        "\n",
        "print(not_bully_df.shape)\n",
        "print(gender_bully_df.shape)\n",
        "print(religion_bully_df.shape)\n",
        "print(age_bully_df.shape)\n",
        "print(ethnicity_bully_df.shape)\n",
        "print(other_bully_df.shape)\n",
        "print(all_bully_df.shape)"
      ],
      "metadata": {
        "id": "yHyA-P1YXi2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "not_bully_df.tail()"
      ],
      "metadata": {
        "id": "ZD49zXcIlvHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sampled_not_bully_df = not_bully_df.sample(n=2000, random_state=42)\n",
        "# sampled_gender_bully_df = gender_bully_df.sample(n=1200, random_state=42)\n",
        "# sampled_age_bully_df = age_bully_df.sample(n=1200, random_state=42)\n",
        "# sampled_religion_bully_df = religion_bully_df.sample(n=1200, random_state=42)\n",
        "# sampled_ethnicity_bully_df = ethnicity_bully_df.sample(n=1200, random_state=42)\n",
        "# sampled_other_bully_df = other_bully_df.sample(n=1200, random_state=42)\n",
        "\n",
        "sampled_not_bully_df = not_bully_df.sample(n=7000, random_state=42)\n",
        "sampled_gender_bully_df = gender_bully_df.sample(n=1750, random_state=42)\n",
        "sampled_age_bully_df = age_bully_df.sample(n=1750, random_state=42)\n",
        "sampled_religion_bully_df = religion_bully_df.sample(n=1750, random_state=42)\n",
        "sampled_ethnicity_bully_df = ethnicity_bully_df.sample(n=1750, random_state=42)\n",
        "sampled_other_bully_df = []\n",
        "\n",
        "sampled2_not_bully_df = not_bully_df.sample(n=3000, random_state=42)\n",
        "sampled2_gender_bully_df = gender_bully_df.sample(n=600, random_state=42)\n",
        "sampled2_age_bully_df = age_bully_df.sample(n=600, random_state=42)\n",
        "sampled2_religion_bully_df = religion_bully_df.sample(n=600, random_state=42)\n",
        "sampled2_ethnicity_bully_df = ethnicity_bully_df.sample(n=600, random_state=42)\n",
        "sampled2_other_bully_df = other_bully_df.sample(n=600, random_state=42)"
      ],
      "metadata": {
        "id": "DSVnf6TiayHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_gender_bully_df.shape"
      ],
      "metadata": {
        "id": "N4wassWhmlxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_gender_bully_df.tail()"
      ],
      "metadata": {
        "id": "tjcB7Thcc9Kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.concat([sampled_not_bully_df, sampled_gender_bully_df, sampled_age_bully_df, sampled_religion_bully_df, sampled_ethnicity_bully_df], ignore_index=True)\n",
        "# final_df['tweet'] = final_df['tweetsplit'].apply(lambda x: ([w for w in x if len(w)>=3]))"
      ],
      "metadata": {
        "id": "l6EbatYVdj5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final2_df = pd.concat([sampled2_not_bully_df, sampled2_gender_bully_df, sampled2_age_bully_df, sampled2_religion_bully_df, sampled2_ethnicity_bully_df, sampled2_other_bully_df], ignore_index=True)\n",
        "# final2_df['tweet'] = final2_df['tweetsplit'].apply(lambda x: ([w for w in x if len(w)>=3]))"
      ],
      "metadata": {
        "id": "KQV57lCVbBL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.tail(20)"
      ],
      "metadata": {
        "id": "akRd22tGt0Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maximum_length=356\n",
        "# xx = pad_sequences(final_df['tweetcnn'], maxlen=maximum_length)\n"
      ],
      "metadata": {
        "id": "T_hdwbz9_3l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final2_df.loc[final2_df['type']!='not_cyberbullying','type'] = 'yes'\n",
        "final2_df.loc[final2_df['type']=='not_cyberbullying','type'] = 'no'"
      ],
      "metadata": {
        "id": "G5IAuGQ6ewbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def allign(text):\n",
        "#   while len(text)<5:\n",
        "#     text.append('<oov>')\n",
        "#   return text\n",
        "\n",
        "# final_df['tweet'] = final_df['tweet'].apply(allign)\n",
        "# final2_df['tweet'] = final2_df['tweet'].apply(allign)"
      ],
      "metadata": {
        "id": "G_m7Hdtsvi80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = {'not_cyberbullying': 0, 'gender': 1, 'age': 1, 'ethnicity': 1, 'religion':1, 'other_cyberbullying':1}\n",
        "mapping2 = {'yes': 1, 'no': 0}"
      ],
      "metadata": {
        "id": "U-CCjtGAoq5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.tail(20)"
      ],
      "metadata": {
        "id": "5vBSnJ_atL0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df['type'] = final_df['type'].map(mapping)\n",
        "final2_df['type'] = final2_df['type'].map(mapping2)"
      ],
      "metadata": {
        "id": "nmFaz-sRpZ6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.tail(20)"
      ],
      "metadata": {
        "id": "1kzlydm-d0mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.shape"
      ],
      "metadata": {
        "id": "MLrxzJXinJIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_texts(sentences):\n",
        "  NUM_WORDS = 30000\n",
        "\n",
        "  # Define/Load Tokenize text function\n",
        "  tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
        "                        lower=True)\n",
        "\n",
        "  # Fit the function on the text\n",
        "  tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "  # Count number of unique tokens\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.'  %len(word_index))\n",
        "  return tokenizer, word_index"
      ],
      "metadata": {
        "id": "RgGk46VTD9vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, word_index = tokenize_texts(final_df['tweet'])\n",
        "final_df['tweetseq'] = tokenizer.texts_to_sequences(final_df['tweet'])"
      ],
      "metadata": {
        "id": "yIyZ4UHdR49d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_df['tweetseq'].dtype)"
      ],
      "metadata": {
        "id": "lzDMMLOhLLeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.tail(20)"
      ],
      "metadata": {
        "id": "p46k3Q__SoM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_df['tweetseq'].shape)"
      ],
      "metadata": {
        "id": "i9FVAhM_OOzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def pad(seq):\n",
        "#   print(seq)\n",
        "#   return pad_sequences(seq,maxlen=150)\n",
        "def padding(sentences):\n",
        "  seq=sentences.tolist()\n",
        "  padded = pad_sequences(seq,maxlen=150,padding='post')\n",
        "  return padded"
      ],
      "metadata": {
        "id": "BL6p_a4RUGcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded = padding(final_df['tweetseq'])"
      ],
      "metadata": {
        "id": "rNuPrr5o5yRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Min value: {padded.min()}, Max value: {padded.max()}\")\n"
      ],
      "metadata": {
        "id": "jwo2uRuA-akL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded.shape"
      ],
      "metadata": {
        "id": "xyrxfor6NvzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df['tweetseq'] = list(padded)"
      ],
      "metadata": {
        "id": "2Y5klznLTDC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.tail(20)"
      ],
      "metadata": {
        "id": "749b7-3wTQqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = dict()\n",
        "\n",
        "# load the whole embedding into memory\n",
        "f = open('./drive/MyDrive/datasets/glove.6B.300d.txt', encoding=\"utf8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_vectors[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(word_vectors))"
      ],
      "metadata": {
        "id": "GZiMF1iO4aZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_WORDS=len(word_index)+1\n",
        "EMBEDDING_DIM=300\n",
        "vocabulary_size=min(len(word_index)+1,(NUM_WORDS))\n",
        "\n",
        "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))"
      ],
      "metadata": {
        "id": "OtIncB-tAH5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wp=[]\n",
        "for word, i in word_index.items():\n",
        "    if i>=NUM_WORDS:\n",
        "        continue\n",
        "    if word in word_vectors:\n",
        "      embedding_vector = word_vectors[word]\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "      fl=0\n",
        "      for x in bad_word:\n",
        "        if x in word:\n",
        "          embedding_vector = word_vectors[x]\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "          fl=1\n",
        "          break\n",
        "      if fl==0 :\n",
        "        wp.append(word)\n",
        "    # try:\n",
        "    #     embedding_vector = word_vectors[word]\n",
        "    #     embedding_matrix[i] = embedding_vector\n",
        "    # except KeyError:\n",
        "    #     wp.append(word)\n",
        "    #     embedding_matrix[i]=np.zeros(300)\n",
        "# ' '.join(wp)"
      ],
      "metadata": {
        "id": "wZcHTQjvAm59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filter_sizes = [1,2]\n",
        "num_filters = 600\n",
        "sequence_length=150\n",
        "FULLY_CONNECT_NUM = 128\n",
        "NUM_CLASSES = 5\n",
        "drop=0.3\n",
        "NUM_WORDS=len(word_index)+1\n",
        "\n",
        "embedding_layer = Embedding(NUM_WORDS,EMBEDDING_DIM,weights=[embedding_matrix],trainable=True)\n",
        "\n",
        "inputs = Input(shape=(sequence_length,))\n",
        "embedding = embedding_layer(inputs)\n",
        "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
        "\n",
        "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "\n",
        "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
        "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
        "\n",
        "merged_tensor = concatenate([maxpool_0, maxpool_1], axis=1)\n",
        "flatten = Flatten()(merged_tensor)\n",
        "dropout = Dropout(drop)(flatten)\n",
        "conc = Dense(40, activation='relu',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
        "dropout1 = Dropout(drop)(conc)\n",
        "\n",
        "# conc1 = Den(40, activation='relu', kernel_regularizer=regularizers.l2(0.01))(dropout1)\n",
        "# dropout2 = Droposeut(drop)(conc1)\n",
        "\n",
        "# conc2 = Dense(10, activation='relu', kernel_regularizer=regularizers.l2(0.01))(dropout2)\n",
        "# dropout3 = Dropout(drop)(conc2)\n",
        "\n",
        "output = Dense(units=1, activation='sigmoid')(dropout1)\n",
        "\n",
        "# this creates a model that includes\n",
        "model = Model(inputs, output)"
      ],
      "metadata": {
        "id": "m5sXcjrAhRTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(final_df['tweetseq'].tolist(), dtype=np.float64)\n",
        "y=final_df['type']\n",
        "yc=y"
      ],
      "metadata": {
        "id": "Lt8XQlVSEkzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape,yc.shape)"
      ],
      "metadata": {
        "id": "bqD75F6SG7Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, yc, test_size=0.30, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)"
      ],
      "metadata": {
        "id": "kBqOrcDCG_Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sen=[\"rape is a crime\"]\n",
        "tdf = pd.DataFrame(sen, columns=[\"sentence\"])\n",
        "tdf[\"sentence\"]=tdf[\"sentence\"].apply(cleantxt)\n",
        "tdf['sentence']=tdf['sentence'].apply(lemmatize_sentence)\n",
        "tdf['senseq'] = tokenizer.texts_to_sequences(tdf['sentence'])\n",
        "pad=padding(tdf['senseq'])\n",
        "tdf['senseq']=list(pad)\n",
        "Xl = np.array(tdf['senseq'].tolist(), dtype=np.float64)\n",
        "Xl"
      ],
      "metadata": {
        "id": "r8xUYK_g7-7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = Adam(learning_rate=1e-3)\n",
        "model.compile(loss='binary_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath='best_model.keras',        # Filepath to save the model\n",
        "    monitor='val_loss',             # Metric to monitor (e.g., 'val_accuracy', 'val_loss')\n",
        "    save_best_only=True,            # Save only the best model\n",
        "    mode='min',                     # 'min' for loss, 'max' for accuracy\n",
        "    verbose=1                       # Display messages\n",
        ")\n",
        "\n",
        "callbacks = [EarlyStopping(monitor='val_loss'),checkpoint]\n",
        "hist_adam = model.fit(X_train, y_train, batch_size=1000, epochs=30, validation_data=(X_val, y_val), callbacks=callbacks)  # starts training"
      ],
      "metadata": {
        "id": "dGICWCLCJrMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.suptitle('Optimizer : Adam', fontsize=10)\n",
        "plt.ylabel('Loss', fontsize=16)\n",
        "plt.xlabel('Epoch', fontsize=14)\n",
        "plt.plot(hist_adam.history['loss'], color='b', label='Training Loss')\n",
        "plt.plot(hist_adam.history['val_loss'], color='r', label='Validation Loss')\n",
        "plt.legend(loc='upper right')"
      ],
      "metadata": {
        "id": "BT796C0yfw0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = load_model('best_model.keras')\n",
        "prob = best_model.predict(X_test)\n",
        "\n",
        "y_pred=[]\n",
        "for xx in prob:\n",
        "  if xx>=0.5:\n",
        "    y_pred.append(1)\n",
        "  else:\n",
        "    y_pred.append(0)\n",
        "\n",
        "# y_pred\n",
        "yy=y_test\n",
        "\n",
        "\n",
        "cnf_matrix = metrics.confusion_matrix(yy, y_pred)\n",
        "cnf_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "6NZ13a0upgjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sen=[\"that's christian\"]\n",
        "tdf = pd.DataFrame(sen, columns=[\"sentence\"])\n",
        "tdf[\"sentence\"]=tdf[\"sentence\"].apply(cleantxt)\n",
        "tdf['sentence']=tdf['sentence'].apply(lemmatize_sentence)\n",
        "tdf['senseq'] = tokenizer.texts_to_sequences(tdf['sentence'])\n",
        "pad=padding(tdf['senseq'])\n",
        "tdf['senseq']=list(pad)\n",
        "Xl = np.array(tdf['senseq'].tolist(), dtype=np.float64)\n",
        "Xl"
      ],
      "metadata": {
        "id": "2jd-FImcokG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob2 = best_model.predict(Xl)"
      ],
      "metadata": {
        "id": "c05-KAfqTmdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob2"
      ],
      "metadata": {
        "id": "lLJIoQAqTrRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "pickl = {\n",
        "    'tokenizer': tokenizer,\n",
        "    'model': best_model\n",
        "}\n",
        "\n",
        "pickle.dump(pickl, open(\"models\"+\".p\",\"wb\"))"
      ],
      "metadata": {
        "id": "A7DqFdebTvxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names=['no_bully', 'bully'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(3,3))\n",
        "\n",
        "# create heatmap\n",
        "sns.heatmap(cnf_matrix, annot=True, cmap=\"YlGnBu\", fmt='g', cbar=False)\n",
        "ax.set_xticklabels(class_names, rotation=45, ha='right')  # Rotate x-ticks for readability\n",
        "ax.set_yticklabels(class_names, rotation=0)\n",
        "ax.xaxis.set_label_position(\"top\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix', y=1.1)\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label(cnn)')\n"
      ],
      "metadata": {
        "id": "L-7kwzAmtvvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_names = ['no_bully', 'bully']\n",
        "print(classification_report(yy, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "IDMCEJ440r9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(yy, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "JC-4hXy5IQeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TP = cnf_matrix[0, 0]\n",
        "FN = cnf_matrix[0, 1]\n",
        "FP = cnf_matrix[1, 0]\n",
        "TN = cnf_matrix[1, 1]\n",
        "\n",
        "recall = TP / (TP + FN)\n",
        "\n",
        "precision = TP / (TP + FP)\n",
        "\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1_score:.2f}\")"
      ],
      "metadata": {
        "id": "eH2ZA_dHU7OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot ROC curve for each class\n",
        "plt.figure(figsize=(6,5))\n",
        "yy=[]\n",
        "yy.append(y_test)\n",
        "yy.append(1-y_test)\n",
        "pr=[]\n",
        "pr.append(prob)\n",
        "pr.append(1-prob)\n",
        "\n",
        "for i in range(2):\n",
        "    fpr, tpr, _ = roc_curve(yy[i], pr[i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'Class {class_names[i]} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "# Plot the diagonal line for random guessing\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guessing')\n",
        "\n",
        "# Label the plot\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Each Class')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "quG61acR1QCS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
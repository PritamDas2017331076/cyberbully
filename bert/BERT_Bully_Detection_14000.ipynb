{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "bbg_R6QGtu81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "1qQvaeq_woO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow\n",
        "!pip install tensorflow==2.12.0\n"
      ],
      "metadata": {
        "id": "6agcr4rlYTyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade tensorflow transformers\n",
        "!pip install transformers==4.31.0"
      ],
      "metadata": {
        "id": "o4Qml6XucAEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLbCI2hLKd7T"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import gensim\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow.compat.v1 as tff\n",
        "# tff.disable_v2_behavior()"
      ],
      "metadata": {
        "id": "_Ose-b3AiGhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WvOv9BHgvroM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "4viiNN9CwmCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "SbW4Sof9zyCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bully_df = pd.read_csv('./drive/MyDrive/datasets/cyberbullying_tweets.csv')"
      ],
      "metadata": {
        "id": "xAtsEEfmKqpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleantxt(txt):\n",
        "    stpw = stopwords.words('english')\n",
        "\n",
        "    stpw.extend(['www','http','utc'])\n",
        "    stpw = set(stpw)\n",
        "    txt = re.sub(r\"\\n\", \" \", txt)\n",
        "    txt = re.sub(\"[\\<\\[].*?[\\>\\]]\", \" \", txt)\n",
        "    txt = txt.lower()\n",
        "    txt = re.sub(r\"[^a-zA-Z ]\", \" \", txt)\n",
        "    txt = re.sub(r\"\\b\\w{1,3}\\b\", \" \",txt)\n",
        "    txt = \" \".join([x for x in txt.split() if x not in stpw])\n",
        "    return txt"
      ],
      "metadata": {
        "id": "L2NPh4W_lyop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bully_df = bully_df.rename(columns={'tweet_text':'tweet','cyberbullying_type':'type'})"
      ],
      "metadata": {
        "id": "rK7eGalzMyI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bully_df['tweet'] = bully_df['tweet'].apply(lambda x: re.sub(r'@\\w+', '', x).strip())"
      ],
      "metadata": {
        "id": "78N_Ju5OmvEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bully_df['tweet'] = bully_df['tweet'].apply(cleantxt)"
      ],
      "metadata": {
        "id": "VYHMlBBbl838"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bully_df['tweet'] = bully_df['tweet'].apply(lambda x: x.lower())\n"
      ],
      "metadata": {
        "id": "Jx43djq9sO4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bully_df['tweet'] = bully_df['tweet'].apply(lambda x: ' '.join([w for w in x.split() if w != 'rt']))\n"
      ],
      "metadata": {
        "id": "hPXynols8xZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bully_df.tail(10)"
      ],
      "metadata": {
        "id": "Ff8MhhkFvErM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# def lemmatize_sentence(sentence):\n",
        "#     tokens = nltk.word_tokenize(sentence)\n",
        "#     lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in tokens]\n",
        "#     return ' '.join(lemmatized_words)"
      ],
      "metadata": {
        "id": "X-PI3cyswfeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bully_df['tweet'] = bully_df['tweet'].apply(lemmatize_sentence)"
      ],
      "metadata": {
        "id": "KW50QMqYxbft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "negative_words = ' '.join([text for text in bully_df['tweet'][bully_df['type'] != 'not_cyberbullying']])\n",
        "wordcloud = WordCloud(width=800, height=500,\n",
        "random_state=21, max_font_size=110).generate(negative_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dizrm6CF2kC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bully_df['tweetsplit'] = bully_df['tweet'].apply(lambda x: x.split())\n",
        "# bully_df['tweet'] = bully_df['tweet'].apply(lambda x: ([w for w in x if len(w)>=3]))\n",
        "# bully_df['tweetcnn'] = bully_df['tweet'].apply(lambda x: ' '.)"
      ],
      "metadata": {
        "id": "QYcyGTAn5RPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gender religion age ethnicity other_cyberbullying\n",
        "not_bully_df = bully_df[bully_df['type'] == 'not_cyberbullying']\n",
        "gender_bully_df = bully_df[bully_df['type'] == 'gender']\n",
        "religion_bully_df = bully_df[bully_df['type'] == 'religion']\n",
        "age_bully_df = bully_df[bully_df['type'] == 'age']\n",
        "ethnicity_bully_df = bully_df[bully_df['type'] == 'ethnicity']\n",
        "other_bully_df = bully_df[bully_df['type'] == 'other_cyberbullying']\n",
        "all_bully_df = bully_df[bully_df['type'] != 'not_cyberbullying']\n",
        "\n",
        "print(not_bully_df.shape)\n",
        "print(gender_bully_df.shape)\n",
        "print(religion_bully_df.shape)\n",
        "print(age_bully_df.shape)\n",
        "print(ethnicity_bully_df.shape)\n",
        "print(other_bully_df.shape)\n",
        "print(all_bully_df.shape)"
      ],
      "metadata": {
        "id": "yHyA-P1YXi2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "not_bully_df.tail()"
      ],
      "metadata": {
        "id": "ZD49zXcIlvHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_not_bully_df = not_bully_df.sample(n=7000, random_state=42)\n",
        "sampled_gender_bully_df = gender_bully_df.sample(n=1750, random_state=42)\n",
        "sampled_age_bully_df = age_bully_df.sample(n=1750, random_state=42)\n",
        "sampled_religion_bully_df = religion_bully_df.sample(n=1750, random_state=42)\n",
        "sampled_ethnicity_bully_df = ethnicity_bully_df.sample(n=1750, random_state=42)\n",
        "sampled_other_bully_df = other_bully_df.sample(n=1200, random_state=42)\n",
        "\n",
        "sampled2_not_bully_df = not_bully_df.sample(n=3000, random_state=42)\n",
        "sampled2_gender_bully_df = gender_bully_df.sample(n=600, random_state=42)\n",
        "sampled2_age_bully_df = age_bully_df.sample(n=600, random_state=42)\n",
        "sampled2_religion_bully_df = religion_bully_df.sample(n=600, random_state=42)\n",
        "sampled2_ethnicity_bully_df = ethnicity_bully_df.sample(n=600, random_state=42)\n",
        "sampled2_other_bully_df = other_bully_df.sample(n=600, random_state=42)"
      ],
      "metadata": {
        "id": "DSVnf6TiayHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_gender_bully_df.shape"
      ],
      "metadata": {
        "id": "N4wassWhmlxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_gender_bully_df.tail()"
      ],
      "metadata": {
        "id": "tjcB7Thcc9Kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.concat([sampled_not_bully_df, sampled_gender_bully_df, sampled_age_bully_df, sampled_religion_bully_df, sampled_ethnicity_bully_df], ignore_index=True)\n",
        "# final_df['tweet'] = final_df['tweetsplit'].apply(lambda x: ([w for w in x if len(w)>=3]))"
      ],
      "metadata": {
        "id": "l6EbatYVdj5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final2_df = pd.concat([sampled2_not_bully_df, sampled2_gender_bully_df, sampled2_age_bully_df, sampled2_religion_bully_df, sampled2_ethnicity_bully_df, sampled2_other_bully_df], ignore_index=True)\n",
        "# final2_df['tweet'] = final2_df['tweetsplit'].apply(lambda x: ([w for w in x if len(w)>=3]))"
      ],
      "metadata": {
        "id": "KQV57lCVbBL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.tail(20)"
      ],
      "metadata": {
        "id": "akRd22tGt0Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final2_df.loc[final2_df['type']!='not_cyberbullying','type'] = 'yes'\n",
        "final2_df.loc[final2_df['type']=='not_cyberbullying','type'] = 'no'"
      ],
      "metadata": {
        "id": "G5IAuGQ6ewbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def allign(text):\n",
        "#   while len(text)<5:\n",
        "#     text.append('<oov>')\n",
        "#   return text\n",
        "\n",
        "# final_df['tweet'] = final_df['tweet'].apply(allign)\n",
        "# final2_df['tweet'] = final2_df['tweet'].apply(allign)"
      ],
      "metadata": {
        "id": "G_m7Hdtsvi80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = {'not_cyberbullying': 0, 'gender': 1, 'age': 1, 'ethnicity': 1, 'religion':1, 'other_cyberbullying':1}\n",
        "mapping2 = {'yes': 1, 'no': 0}"
      ],
      "metadata": {
        "id": "U-CCjtGAoq5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.tail(20)"
      ],
      "metadata": {
        "id": "5vBSnJ_atL0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df['type'] = final_df['type'].map(mapping)\n",
        "final2_df['type'] = final2_df['type'].map(mapping2)"
      ],
      "metadata": {
        "id": "nmFaz-sRpZ6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.tail(20)"
      ],
      "metadata": {
        "id": "1kzlydm-d0mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.shape"
      ],
      "metadata": {
        "id": "MLrxzJXinJIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.tail(20)"
      ],
      "metadata": {
        "id": "C6yJ6ChuJW7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.shape"
      ],
      "metadata": {
        "id": "J389EPajF-we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=final_df['tweet']\n",
        "y=final_df['type']"
      ],
      "metadata": {
        "id": "78Pt--9JFcpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)"
      ],
      "metadata": {
        "id": "Y9LpTJ0QT_R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "max_len=120\n",
        "X_train_encoded = tokenizer.batch_encode_plus(X_train.tolist(),\n",
        "                                              padding=True,\n",
        "                                              truncation=True,\n",
        "                                              max_length = max_len,\n",
        "                                              return_tensors='tf')\n",
        "\n",
        "X_val_encoded = tokenizer.batch_encode_plus(X_val.tolist(),\n",
        "                                              padding=True,\n",
        "                                              truncation=True,\n",
        "                                              max_length = max_len,\n",
        "                                              return_tensors='tf')\n",
        "\n",
        "X_test_encoded = tokenizer.batch_encode_plus(X_test.tolist(),\n",
        "                                              padding=True,\n",
        "                                              truncation=True,\n",
        "                                              max_length = max_len,\n",
        "                                              return_tensors='tf')"
      ],
      "metadata": {
        "id": "cl-Dbs24UIFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train\n",
        "# y[0]"
      ],
      "metadata": {
        "id": "x7RWVEmDVMHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_encoded['input_ids'][0]"
      ],
      "metadata": {
        "id": "lNl87keuXqyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(X_train_encoded['input_ids'][0])"
      ],
      "metadata": {
        "id": "sMV3_UrwZ77D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_encoded['attention_mask'][0]"
      ],
      "metadata": {
        "id": "uLnoyUAHaSBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
        "learning_rate = 2e-5\n",
        "# we will do just 1 epoch, though multiple epochs might be better as long as we will not overfit the model\n",
        "number_of_epochs = 1\n",
        "# model initialization\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2,)\n",
        "# The number of epochs is set to 2 as higher epochs will give rise to overfitting problems as well as take more time for the model to train.\n",
        "# choosing Adam optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
        "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "metadata": {
        "id": "XrsnMzmRafyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "callbacks = [EarlyStopping(monitor='val_loss')]\n",
        "history = model.fit(\n",
        "    [X_train_encoded['input_ids'], X_train_encoded['token_type_ids'], X_train_encoded['attention_mask']],\n",
        "    y_train,\n",
        "    validation_data=(\n",
        "      [X_val_encoded['input_ids'], X_val_encoded['token_type_ids'], X_val_encoded['attention_mask']],y_val),\n",
        "    batch_size=50,\n",
        "    epochs=3,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "zFKtZNYLtk1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.suptitle('Optimizer : Adam', fontsize=10)\n",
        "plt.ylabel('Loss', fontsize=16)\n",
        "plt.xlabel('Epoch', fontsize=14)\n",
        "plt.plot(history.history['loss'], color='b', label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], color='r', label='Validation Loss')\n",
        "plt.legend(loc='upper right')"
      ],
      "metadata": {
        "id": "wnw1vLwEMpxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict([X_test_encoded['input_ids'], X_test_encoded['token_type_ids'], X_test_encoded['attention_mask']])\n",
        "y_pred"
      ],
      "metadata": {
        "id": "JEtNKZDV68Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yy_pred=[]\n",
        "for xx in y_pred.logits:\n",
        "  yy_pred.append(np.argmax(xx))\n"
      ],
      "metadata": {
        "id": "AXwC9gDBND6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnf_matrix = metrics.confusion_matrix(y_test, yy_pred)\n",
        "cnf_matrix"
      ],
      "metadata": {
        "id": "rinnRjBMNSd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names=['no_bully', 'bully'] # name  of classes\n",
        "fig, ax = plt.subplots(figsize=(3,3))\n",
        "\n",
        "# create heatmap\n",
        "sns.heatmap(cnf_matrix, annot=True, cmap=\"YlGnBu\", fmt='g', cbar=False)\n",
        "ax.set_xticklabels(class_names, rotation=45, ha='right')  # Rotate x-ticks for readability\n",
        "ax.set_yticklabels(class_names, rotation=0)\n",
        "ax.xaxis.set_label_position(\"top\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix', y=1.1)\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label(bert)')"
      ],
      "metadata": {
        "id": "ypkCMtIh6_pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_names = ['no_bully', 'bully']\n",
        "print(classification_report(y_test, yy_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "P-DFH8t57DLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_score = y_pred.logits\n",
        "print(y_score)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "y_test_one_hot = []\n",
        "a1=[]\n",
        "a1.append(1)\n",
        "a1.append(0)\n",
        "a2=[]\n",
        "a2.append(0)\n",
        "a2.append(1)\n",
        "for x in y_test:\n",
        "  if x==0:\n",
        "    y_test_one_hot.append([1,0])\n",
        "  else:\n",
        "    y_test_one_hot.append([0,1])\n",
        "yt1=np.array(y_test_one_hot)\n",
        "print(yt1[:, 0])\n",
        "yt2=np.array(y_score)\n",
        "\n",
        "for i in range(2):\n",
        "    fpr, tpr, _ = roc_curve(yt1[:, i], yt2[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guessing')\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Each Class')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tLIhY8TX7GpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logacc = accuracy_score(y_test, yy_pred)\n",
        "print(f\"Model Accuracy: {logacc:.2f}\")"
      ],
      "metadata": {
        "id": "PcgtWGdq7Lwa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}